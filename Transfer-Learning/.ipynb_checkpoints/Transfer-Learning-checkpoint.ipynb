{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"hw3.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0bd8ea6cc9294b15baca1d79b841a4ae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6ac479b212e04623a8d633199334fa3b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e90b638331994e16b1d4162cadf4db06","IPY_MODEL_750e910f6b434d13934dc2446e7e4c79"]}},"6ac479b212e04623a8d633199334fa3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e90b638331994e16b1d4162cadf4db06":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_eb2e2c17c95f4765bf32eef263019d5a","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":46827520,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":46827520,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9948d96087c94e66a06ee0534a13ce69"}},"750e910f6b434d13934dc2446e7e4c79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9e01f58d323943f29fde92454ef4631e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44.7M/44.7M [00:01&lt;00:00, 30.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2ac086c198fe484d978a2ef925a01d28"}},"eb2e2c17c95f4765bf32eef263019d5a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9948d96087c94e66a06ee0534a13ce69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9e01f58d323943f29fde92454ef4631e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2ac086c198fe484d978a2ef925a01d28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"3kt5uUbQl1in"},"source":["---   \n","# HW3 - Transfer learning\n","\n","#### Due October 30, 2019\n","\n","In this assignment you will learn about transfer learning. This technique is perhaps one of the most important techniques for industry. When a problem you want to solve does not have enough data, we use a different (larger) dataset to learn representations which can help us solve our task using the smaller task.\n","\n","The general steps to transfer learning are as follows:\n","\n","1. Find a huge dataset with similar characteristics to the problem you are interested in.\n","2. Choose a model powerful enough to extract meaningful representations from the huge dataset.\n","3. Train this model on the huge dataset.\n","4. Use this model to train on the smaller dataset.\n","\n","\n","### This homework has the following sections:\n","1. Question 1: MNIST fine-tuning (Parts A, B, C, D).\n","2. Question 2: Pretrain on Wikitext2 (Part A, B, C, D)\n","3. Question 3: Finetune on MNLI (Part A, B, C, D)\n","4. Question 4: Finetune using pretrained BERT (Part A, B, C)"]},{"cell_type":"markdown","metadata":{"id":"j3HN1M6fl1ip"},"source":["---   \n","## Question 1 (MNIST transfer learning)\n","To grasp the high-level approach to transfer learning, let's first do a simple example using computer vision. \n","\n","The torchvision library has pretrained models (resnets, vggnets, etc) on the Imagenet dataset. Imagenet is a dataset\n","with 1.3 million images covering over 1000 classes of objects. When you use one of these models, the weights of the model initialize\n","with the weights saved from training on imagenet.\n","\n","In this task we will:\n","1. Choose a pretrained model.\n","2. Freeze the model so that the weights don't change.\n","3. Fine-tune on a few labels of MNIST.   "]},{"cell_type":"markdown","metadata":{"id":"ixtpp6mTl1iq"},"source":["#### Choose a model\n","Here we pick any of the models from torchvision"]},{"cell_type":"code","metadata":{"id":"S6aQVNqRl1iq","colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["0bd8ea6cc9294b15baca1d79b841a4ae","6ac479b212e04623a8d633199334fa3b","e90b638331994e16b1d4162cadf4db06","750e910f6b434d13934dc2446e7e4c79","eb2e2c17c95f4765bf32eef263019d5a","9948d96087c94e66a06ee0534a13ce69","9e01f58d323943f29fde92454ef4631e","2ac086c198fe484d978a2ef925a01d28"]},"executionInfo":{"status":"ok","timestamp":1610222473118,"user_tz":300,"elapsed":6147,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"259368b1-82ab-421b-d9f3-268838cbae12"},"source":["import torch\n","import torchvision.models as models\n","\n","class Identity(torch.nn.Module):\n","    def __init__(self):\n","        super(Identity, self).__init__()\n","        \n","    def forward(self, x):\n","        return x\n","\n","# init the pretrained feature extractor\n","pretrained_resnet18 = models.resnet18(pretrained=True)\n","\n","# we don't want the built in last layer, we're going to modify it ourselves\n","pretrained_resnet18.fc = Identity()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0bd8ea6cc9294b15baca1d79b841a4ae","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9YDESoLEl1it"},"source":["#### Freeze the model\n","Here we freeze the weights of the model. Freezing means the gradients will not backpropagate\n","into these weights.\n","\n","By doing this you can think about the model as a feature extractor. This feature extractor outputs\n","a **representation** of an input. This representation is a matrix that encodes information about the input."]},{"cell_type":"code","metadata":{"id":"0Nrrwgc_l1iu","executionInfo":{"status":"ok","timestamp":1610222473119,"user_tz":300,"elapsed":6142,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}}},"source":["def freeze_model(model):\n","    for param in model.parameters():\n","        param.requires_grad = False\n","        \n","def unfreeze_model(model):\n","    for param in model.parameters():\n","        param.requires_grad = True\n","        \n","freeze_model(pretrained_resnet18)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SR21Q6A6l1iw"},"source":["#### Init target dataset\n","Here we define the dataset we are actually interested in."]},{"cell_type":"code","metadata":{"id":"KSux0qz4l1ix"},"source":["import os\n","from torchvision import transforms\n","from torchvision.datasets import  MNIST\n","from torch.utils.data import DataLoader, random_split\n","import torch.nn.functional as F\n","\n","#  train/val  split\n","mnist_dataset = MNIST(os.getcwd(), train=True, download=True, transform = transforms.Compose([transforms.Grayscale(3), transforms.ToTensor()]))\n","mnist_train, mnist_val = random_split(mnist_dataset, [55000, 5000])\n","\n","mnist_train = DataLoader(mnist_train, batch_size=32)\n","mnist_val = DataLoader(mnist_val, batch_size=32)\n","\n","# test split\n","mnist_test = MNIST(os.getcwd(), train=False, download=True, transform = transforms.Compose([transforms.Grayscale(3), transforms.ToTensor()]))\n","mnist_test = DataLoader(mnist_test, batch_size=32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J1JxXYmRl1iz"},"source":["### Part A (init fine-tune model)\n","decide what model to use for fine-tuning"]},{"cell_type":"code","metadata":{"id":"M69KgGEll1i0"},"source":["def init_fine_tune_model():\n","    # YOUR CODE HERE\n","    fine_tune_model = feature_extractor\n","    return fine_tune_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Jb3MZ2Sl1i2"},"source":["### Part B (Fine-tune (Frozen))\n","\n","The actual problem we care about solving likely has a different number of classes or is a different task altogether. Fine-tuning is the process of using the extracted representations (features) to solve this downstream task  (the task you're interested in).\n","\n","To illustrate this, we'll use our pretrained model (on Imagenet), to solve the MNIST classification task.\n","\n","There are two types of finetuning. \n","\n","#### 1. Frozen feature_extractor\n","In the first type we pretrain with the FROZEN feature_extractor and NEVER unfreeze it during finetuning.\n","\n","\n","#### 2. Unfrozen feature_extractor\n","In the second, we finetune with a FROZEN feature_extractor for a few epochs, then unfreeze the feature extractor and finish training.\n","\n","\n","In this part we will use the first version"]},{"cell_type":"code","metadata":{"id":"zCamCMUal1i3"},"source":["import torch.optim as optim\n","from torch import nn\n","from copy import deepcopy\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","def FROZEN_fine_tune_mnist(feature_extractor, fine_tune_model, mnist_train, mnist_val):\n","    \"\"\"\n","    model is a feature extractor (resnet).\n","    Create a new model which uses those features to finetune on MNIST\n","    \n","    return the fine_tune model\n","    \"\"\"     \n","    \n","    # INSERT YOUR CODE: (train the fine_tune model using features extracted by feature_extractor)\n","    for param in feature_extractor.parameters():\n","        param.requires_grad = False\n","    feature_extractor.fc = nn.Linear(512, 10)\n","    feature_extractor.to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(feature_extractor.parameters(), lr = 0.0001)\n","    #do train\n","    highest_acc = 0\n","    for epoch in range(30):\n","        feature_extractor.train()\n","        correct_train = 0\n","        total_train = 0\n","        correct_val = 0\n","        total_val = 0\n","        for i, (imgs, labels) in enumerate(mnist_train):\n","            optimizer.zero_grad()\n","            imgs = imgs.to(device)\n","            labels = labels.to(device)\n","            outputs = feature_extractor(imgs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            predicted = outputs.max(1, keepdim=True)[1]\n","            total_train += labels.size(0)\n","            correct_train += predicted.eq(labels.view_as(predicted)).sum().item()\n","        print('Training accuracy after {} epoch = {:.{prec}f}'.format(epoch, 100 * correct_train / total_train, prec=4))\n","            \n","        # do eval\n","        feature_extractor.eval()\n","        with torch.no_grad():\n","            for i, (imgs, labels) in enumerate(mnist_val):\n","                imgs = imgs.to(device)\n","                labels = labels.to(device)\n","                outputs = feature_extractor(imgs)\n","                predicted = outputs.max(1, keepdim=True)[1]\n","                total_val += labels.size(0)\n","                correct_val += predicted.eq(labels.view_as(predicted)).sum().item()\n","                loss = criterion(outputs, labels)\n","        print('Validation accuracy after {} epoch = {:.{prec}f}'.format(epoch, 100 * correct_val / total_val, prec=4))\n","        if 100 * correct_val / total_val > highest_acc:\n","                highest_acc = 100 * correct_val / total_val\n","                fine_tune_model = deepcopy(feature_extractor)\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7XbJt4vSl1i5"},"source":["### Part C (compute test accuracy)\n","Compute the test accuracy of fine-tuned model on MNIST"]},{"cell_type":"code","metadata":{"id":"KW8NZ5vel1i5"},"source":["def calculate_mnist_test_accuracy(feature_extractor, fine_tune_model, mnist_test):\n","    fine_tune_model.eval()\n","    total_test, correct_test = 0, 0\n","    criterion = nn.CrossEntropyLoss()\n","    with torch.no_grad():\n","        for i, (imgs, labels) in enumerate(mnist_test):\n","            imgs = imgs.to(device)\n","            labels = labels.to(device)\n","            outputs = fine_tune_model(imgs)\n","            predicted = outputs.max(1, keepdim=True)[1]\n","            total_test += labels.size(0)\n","            correct_test += predicted.eq(labels.view_as(predicted)).sum().item()\n","            loss = criterion(outputs, labels)\n","    test_accuracy = 100 * correct_test / total_test\n","    return test_accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EDSp0cKzl1i7"},"source":["### Grade!\n","Let's see how you did"]},{"cell_type":"code","metadata":{"id":"bJD-mXSwl1i8","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1571631944112,"user_tz":240,"elapsed":298810,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"228b5fc2-5a6c-4b6d-81fb-b974543a378c"},"source":["def grade_mnist_frozen():\n","    \n","    # init a ft model\n","    fine_tune_model = init_fine_tune_model()\n","    \n","    # run the transfer learning routine\n","    FROZEN_fine_tune_mnist(pretrained_resnet18, fine_tune_model, mnist_train, mnist_val)\n","    \n","    # calculate test accuracy\n","    test_accuracy = calculate_mnist_test_accuracy(pretrained_resnet18, fine_tune_model, mnist_test)\n","    \n","    # the real threshold will be released by Oct 11 \n","    assert test_accuracy > 0.0, 'your accuracy is too low...'\n","    \n","    return test_accuracy\n","    \n","frozen_test_accuracy = grade_mnist_frozen()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training accuracy after 0 epoch = 52.2527\n","Validation accuracy after 0 epoch = 67.8000\n","Training accuracy after 1 epoch = 67.3145\n","Validation accuracy after 1 epoch = 72.1400\n","Training accuracy after 2 epoch = 70.1982\n","Validation accuracy after 2 epoch = 74.1800\n","Training accuracy after 3 epoch = 71.5491\n","Validation accuracy after 3 epoch = 75.1800\n","Training accuracy after 4 epoch = 72.4091\n","Validation accuracy after 4 epoch = 75.7600\n","Training accuracy after 5 epoch = 72.9673\n","Validation accuracy after 5 epoch = 76.2800\n","Training accuracy after 6 epoch = 73.4473\n","Validation accuracy after 6 epoch = 76.5400\n","Training accuracy after 7 epoch = 73.7582\n","Validation accuracy after 7 epoch = 76.7800\n","Training accuracy after 8 epoch = 74.0382\n","Validation accuracy after 8 epoch = 77.0400\n","Training accuracy after 9 epoch = 74.2145\n","Validation accuracy after 9 epoch = 77.1000\n","Training accuracy after 10 epoch = 74.3782\n","Validation accuracy after 10 epoch = 77.2800\n","Training accuracy after 11 epoch = 74.5600\n","Validation accuracy after 11 epoch = 77.2400\n","Training accuracy after 12 epoch = 74.7109\n","Validation accuracy after 12 epoch = 77.1200\n","Training accuracy after 13 epoch = 74.8364\n","Validation accuracy after 13 epoch = 77.2200\n","Training accuracy after 14 epoch = 74.9091\n","Validation accuracy after 14 epoch = 77.3600\n","Training accuracy after 15 epoch = 74.9945\n","Validation accuracy after 15 epoch = 77.3800\n","Training accuracy after 16 epoch = 75.1545\n","Validation accuracy after 16 epoch = 77.5400\n","Training accuracy after 17 epoch = 75.2527\n","Validation accuracy after 17 epoch = 77.4800\n","Training accuracy after 18 epoch = 75.3127\n","Validation accuracy after 18 epoch = 77.5000\n","Training accuracy after 19 epoch = 75.3745\n","Validation accuracy after 19 epoch = 77.5600\n","Training accuracy after 20 epoch = 75.4236\n","Validation accuracy after 20 epoch = 77.5400\n","Training accuracy after 21 epoch = 75.4636\n","Validation accuracy after 21 epoch = 77.6200\n","Training accuracy after 22 epoch = 75.5255\n","Validation accuracy after 22 epoch = 77.6800\n","Training accuracy after 23 epoch = 75.5582\n","Validation accuracy after 23 epoch = 77.7000\n","Training accuracy after 24 epoch = 75.6018\n","Validation accuracy after 24 epoch = 77.6800\n","Training accuracy after 25 epoch = 75.6600\n","Validation accuracy after 25 epoch = 77.7600\n","Training accuracy after 26 epoch = 75.6982\n","Validation accuracy after 26 epoch = 77.7000\n","Training accuracy after 27 epoch = 75.7364\n","Validation accuracy after 27 epoch = 77.7200\n","Training accuracy after 28 epoch = 75.7691\n","Validation accuracy after 28 epoch = 77.7200\n","Training accuracy after 29 epoch = 75.8091\n","Validation accuracy after 29 epoch = 77.7800\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iF6bVfDBurTO","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1571631979976,"user_tz":240,"elapsed":894,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"25c1589b-c3b5-4fbe-cc66-94075447c3f0"},"source":["frozen_test_accuracy"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["78.45"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"R9tqmhEzl1i-"},"source":["### Part D (Fine-tune Unfrozen)\n","Now we'll learn how to train using the \"unfrozen\" approach.\n","\n","In this approach we'll:\n","1. keep the feature_extract frozen for a few epochs (10)\n","2. Unfreeze it.\n","3. Finish training"]},{"cell_type":"code","metadata":{"id":"_hkivTcFl1i-"},"source":["def UNFROZEN_fine_tune_mnist(feature_extractor, fine_tune_model, mnist_train, mnist_val):\n","    \"\"\"\n","    model is a feature extractor (resnet).\n","    Create a new model which uses those features to finetune on MNIST\n","    \n","    return the fine_tune model\n","    \"\"\"     \n","    \n","    # INSERT YOUR CODE:\n","    # keep frozen for 10 epochs\n","    # ... train\n","    # unfreeze\n","    # train for rest of the time\n","    for param in feature_extractor.parameters():\n","        param.requires_grad = False\n","    feature_extractor.fc = nn.Linear(512, 10)\n","    feature_extractor.to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(feature_extractor.parameters(), lr = 0.00001)\n","    #do train\n","    highest_acc = 0\n","    for epoch in range(10):\n","        correct_train = 0\n","        total_train = 0\n","        correct_val = 0\n","        total_val = 0\n","        feature_extractor.train()\n","        for i, (imgs, labels) in enumerate(mnist_train):\n","            optimizer.zero_grad()\n","            imgs = imgs.to(device)\n","            labels = labels.to(device)\n","            outputs = feature_extractor(imgs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            predicted = outputs.max(1, keepdim=True)[1]\n","            total_train += labels.size(0)\n","            correct_train += predicted.eq(labels.view_as(predicted)).sum().item()\n","        print('Training accuracy after {} epoch = {:.{prec}f}'.format(epoch, 100 * correct_train / total_train, prec=4))\n","        # do eval\n","        feature_extractor.eval()\n","        with torch.no_grad():\n","            for i, (imgs, labels) in enumerate(mnist_val):\n","                imgs = imgs.to(device)\n","                labels = labels.to(device)\n","                outputs = feature_extractor(imgs)\n","                predicted = outputs.max(1, keepdim=True)[1]\n","                total_val += labels.size(0)\n","                correct_val += predicted.eq(labels.view_as(predicted)).sum().item()\n","                loss = criterion(outputs, labels)\n","                if 100 * correct_val / total_val > highest_acc:\n","                    highest_acc = 100 * correct_val / total_val\n","                    fine_tune_model = deepcopy(feature_extractor)\n","        print('Validation accuracy after {} epoch = {:.{prec}f}'.format(epoch, 100 * correct_val / total_val, prec=4))\n","    for param in feature_extractor.parameters():\n","        param.requires_grad = True\n","    for epoch in range(20):\n","        correct_train = 0\n","        total_train = 0\n","        correct_val = 0\n","        total_val = 0\n","        feature_extractor.train()\n","        for i, (imgs, labels) in enumerate(mnist_train):\n","            optimizer.zero_grad()\n","            imgs = imgs.to(device)\n","            labels = labels.to(device)\n","            outputs = feature_extractor(imgs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            predicted = outputs.max(1, keepdim=True)[1]\n","            total_train += labels.size(0)\n","            correct_train += predicted.eq(labels.view_as(predicted)).sum().item()\n","        print('Training accuracy after {} epoch = {:.{prec}f}'.format(epoch + 10, 100 * correct_train / total_train, prec=4))\n","        # do eval\n","        feature_extractor.eval()\n","        with torch.no_grad():\n","            for i, (imgs, labels) in enumerate(mnist_val):\n","                imgs = imgs.to(device)\n","                labels = labels.to(device)\n","                outputs = feature_extractor(imgs)\n","                predicted = outputs.max(1, keepdim=True)[1]\n","                total_val += labels.size(0)\n","                correct_val += predicted.eq(labels.view_as(predicted)).sum().item()\n","                loss = criterion(outputs, labels)\n","                if 100 * correct_val / total_val > highest_acc:\n","                    highest_acc = 100 * correct_val / total_val\n","                    fine_tune_model = deepcopy(feature_extractor)\n","        print('Validation accuracy after {} epoch = {:.{prec}f}'.format(epoch + 10, 100 * correct_val / total_val, prec=4))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L7M7VBABl1jA"},"source":["### Grade UNFROZEN\n","Let's see if there's a difference in accuracy!"]},{"cell_type":"code","metadata":{"id":"sur8tYKdl1jB","colab":{"base_uri":"https://localhost:8080/","height":697},"executionInfo":{"status":"ok","timestamp":1571634349676,"user_tz":240,"elapsed":1711037,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"2e09f3a1-f8e5-4dd9-e87c-8b9dcf38314a"},"source":["def grade_mnist_unfrozen():\n","    \n","    # init a ft model\n","    fine_tune_model = init_fine_tune_model()\n","    \n","    # run the transfer learning routine\n","    UNFROZEN_fine_tune_mnist(pretrained_resnet18, fine_tune_model, mnist_train, mnist_val)\n","    \n","    # calculate test accuracy\n","    test_accuracy = calculate_mnist_test_accuracy(pretrained_resnet18, fine_tune_model, mnist_test)\n","    \n","    # the real threshold will be released by Oct 11 \n","    assert test_accuracy > 0.0, 'your accuracy is too low...'\n","    \n","    return test_accuracy\n","    \n","unfrozen_test_accuracy = grade_mnist_unfrozen()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training accuracy after 0 epoch = 19.3218\n","Validation accuracy after 0 epoch = 30.9200\n","Training accuracy after 1 epoch = 37.1855\n","Validation accuracy after 1 epoch = 45.9400\n","Training accuracy after 2 epoch = 47.7109\n","Validation accuracy after 2 epoch = 54.3200\n","Training accuracy after 3 epoch = 53.6727\n","Validation accuracy after 3 epoch = 58.9800\n","Training accuracy after 4 epoch = 57.2782\n","Validation accuracy after 4 epoch = 62.3800\n","Training accuracy after 5 epoch = 59.7345\n","Validation accuracy after 5 epoch = 64.2800\n","Training accuracy after 6 epoch = 61.5836\n","Validation accuracy after 6 epoch = 65.9000\n","Training accuracy after 7 epoch = 62.9618\n","Validation accuracy after 7 epoch = 66.9800\n","Training accuracy after 8 epoch = 64.0964\n","Validation accuracy after 8 epoch = 68.0800\n","Training accuracy after 9 epoch = 65.0364\n","Validation accuracy after 9 epoch = 68.8200\n","Training accuracy after 0 epoch = 89.6564\n","Validation accuracy after 0 epoch = 97.1800\n","Training accuracy after 1 epoch = 97.0436\n","Validation accuracy after 1 epoch = 98.1000\n","Training accuracy after 2 epoch = 99.0545\n","Validation accuracy after 2 epoch = 98.5800\n","Training accuracy after 3 epoch = 99.7618\n","Validation accuracy after 3 epoch = 98.6400\n","Training accuracy after 4 epoch = 99.9673\n","Validation accuracy after 4 epoch = 98.6600\n","Training accuracy after 5 epoch = 99.9909\n","Validation accuracy after 5 epoch = 98.7600\n","Training accuracy after 6 epoch = 99.9982\n","Validation accuracy after 6 epoch = 98.8400\n","Training accuracy after 7 epoch = 99.9109\n","Validation accuracy after 7 epoch = 98.7800\n","Training accuracy after 8 epoch = 99.9836\n","Validation accuracy after 8 epoch = 98.8200\n","Training accuracy after 9 epoch = 100.0000\n","Validation accuracy after 9 epoch = 98.8200\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0YQkhxntuvEm","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1571634349679,"user_tz":240,"elapsed":12,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"0a8cf800-e775-4006-e138-a0b225d661af"},"source":["unfrozen_test_accuracy"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["98.79"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"parvrY3ll1jD"},"source":["assert unfrozen_test_accuracy > frozen_test_accuracy, 'the unfrozen model should be better'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fbx9FaBol1jF"},"source":["--- \n","# Question 2 (train a model on Wikitext-2)\n","\n","Here we'll apply what we just learned to NLP. In this section we'll make our own feature extractor and pretrain it on Wikitext-2.\n","\n","The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\n","\n","#### Part A\n","In this section you need to generate the training, validation and test split. Feel free to use code from your previous lectures."]},{"cell_type":"code","metadata":{"id":"lVN8XTkvMH3C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610222511906,"user_tz":300,"elapsed":22154,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"bc8d59e9-160c-4a00-fec4-6a7e2cb1cc5f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oJ7Oegj-K77-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610222516522,"user_tz":300,"elapsed":4597,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"bb345104-c965-4b15-f35d-3fe36dc91697"},"source":["!pip install jsonlines"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting jsonlines\n","  Downloading https://files.pythonhosted.org/packages/d4/58/06f430ff7607a2929f80f07bfd820acbc508a4e977542fefcc522cde9dff/jsonlines-2.0.0-py3-none-any.whl\n","Installing collected packages: jsonlines\n","Successfully installed jsonlines-2.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4tj_fNjly4by","executionInfo":{"status":"ok","timestamp":1610222516523,"user_tz":300,"elapsed":4595,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}}},"source":["import torch\n","from torch.utils.data import Dataset\n","from tqdm import tqdm\n","import pickle\n","import torch\n","import torchvision.models as models\n","import os\n","from torchvision import transforms\n","from torchvision.datasets import  MNIST\n","from torch.utils.data import DataLoader, random_split\n","import torch.nn.functional as F\n","import os\n","import json\n","import jsonlines\n","import numpy as np\n","from collections import defaultdict\n","from torch import nn"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"cKpzwgzkQYqm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610222646175,"user_tz":300,"elapsed":134240,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"e5f9dfa0-997b-46af-a798-b50839a79f22"},"source":["import io\n","def load_vectors(fname):\n","    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n","    n, d = map(int, fin.readline().split())\n","    embedding_size = 300\n","    max_vocab_size = 35000\n","    embedding_dict = np.random.randn(max_vocab_size+2, embedding_size)\n","    all_train_tokens = []\n","    i = 0\n","    \n","    for line in fin:\n","        tokens = line.rstrip().split(' ')\n","        all_train_tokens.append(tokens[0])\n","        embedding_dict[i+2] = list(map(float, tokens[1:]))\n","        i += 1\n","        if i == max_vocab_size:\n","            break\n","            \n","    return embedding_dict, all_train_tokens\n","  \n","# download the vectors yourself\n","fasttext_embedding_dict, all_fasttext_tokens = load_vectors('/content/drive/My Drive/wiki-news-300d-1M.vec')\n","  \n","class LMDataset(Dataset):\n","    def __init__(self, list_of_token_lists):\n","        self.input_tensors = []\n","        self.target_tensors = []\n","\n","        for sample in list_of_token_lists:\n","            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n","            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n","\n","    def __len__(self):\n","        return len(self.input_tensors)\n","\n","    def __getitem__(self, idx):\n","        return (self.input_tensors[idx], self.target_tensors[idx])\n","\n","\n","def tokenize_dataset(datasets, dictionary):\n","    tokenized_datasets = {}\n","    for split, dataset in datasets.items():\n","        _current_dictified = []\n","        for l in tqdm(dataset):\n","            l = ['<bos>'] + l + ['<eos>']\n","            encoded_l = dictionary.encode_token_seq(l)\n","            _current_dictified.append(encoded_l)\n","        tokenized_datasets[split] = _current_dictified\n","    return tokenized_datasets\n","\n","def tokenize_mnli_dataset(datasets, dictionary):\n","    tokenized_datasets = {}\n","    for split, dataset in datasets.items():\n","        _current_dictified = []\n","        for s1, s2 in tqdm(dataset):\n","            s1 = ['<bos>'] + s1 + ['<eos>']\n","            s2 = ['<bos>'] + s2 + ['<eos>']\n","            encoded_s1 = dictionary.encode_token_seq(s1)            \n","            encoded_s2 = dictionary.encode_token_seq(s2)\n","            _current_dictified.append([encoded_s1, encoded_s2])\n","        tokenized_datasets[split] = _current_dictified\n","    return tokenized_datasets\n","\n","def pad_list_of_tensors(list_of_tensors, pad_token):\n","    max_length = max([t.size(-1) for t in list_of_tensors])\n","    padded_list = []\n","    for t in list_of_tensors:\n","        padded_tensor = torch.cat(\n","            [t, torch.tensor([[pad_token] * (max_length - t.size(-1))], dtype=torch.long)], dim=-1)\n","        padded_list.append(padded_tensor)\n","\n","    padded_tensor = torch.cat(padded_list, dim=0)\n","    return padded_tensor\n","\n","\n","def pad_collate_fn(pad_idx, batch):\n","    input_list = [s[0] for s in batch]\n","    target_list = [s[1] for s in batch]\n","    input_tensor = pad_list_of_tensors(input_list, pad_idx)\n","    target_tensor = pad_list_of_tensors(target_list, pad_idx)\n","    return input_tensor, target_tensor\n","\n","\n","def load_wikitext(data_dir):\n","    import subprocess\n","    filename = os.path.join(data_dir, 'wikitext2-sentencized.json')\n","    if not os.path.exists(filename):\n","        os.makedirs(data_dir, exist_ok=True)\n","        url = \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\"\n","        args = ['wget', '-O', filename, url]\n","        subprocess.call(args)\n","    raw_datasets = json.load(open(filename, 'r'))\n","    for name in raw_datasets:\n","        raw_datasets[name] = [x.split() for x in raw_datasets[name]]\n","\n","    if os.path.exists(os.path.join(data_dir, 'vocab.pkl')):\n","        vocab = pickle.load(open(os.path.join(data_dir, 'vocab.pkl'), 'rb'))\n","    else:\n","        vocab = Dictionary(raw_datasets, include_valid=False)\n","        pickle.dump(vocab, open(os.path.join(data_dir, 'vocab.pkl'), 'wb'))\n","\n","    tokenized_datasets = tokenize_dataset(raw_datasets, vocab)\n","    datasets = {name: LMDataset(ds) for name, ds in tokenized_datasets.items()}\n","    print(\"Vocab size: %d\" % (len(vocab)))\n","    return raw_datasets, datasets, vocab\n","\n","\n","class Dictionary(object):\n","    def __init__(self, datasets, include_valid=False):\n","        self.tokens = []\n","        self.ids = {}\n","        self.counts = {}\n","        \n","        # add special tokens\n","        self.add_token('<bos>')\n","        self.add_token('<eos>')\n","        self.add_token('<pad>')\n","        self.add_token('<unk>')\n","        \n","        for line in tqdm(datasets['train']):\n","            for w in line:\n","                self.add_token(w)\n","                    \n","        if include_valid is True:\n","            for line in tqdm(datasets['valid']):\n","                for w in line:\n","                    self.add_token(w)\n","                            \n","    def add_token(self, w):\n","        if w not in self.tokens:\n","            self.tokens.append(w)\n","            _w_id = len(self.tokens) - 1\n","            self.ids[w] = _w_id\n","            self.counts[w] = 1\n","        else:\n","            self.counts[w] += 1\n","\n","    def get_id(self, w):\n","        return self.ids[w]\n","    \n","    def get_token(self, idx):\n","        return self.tokens[idx]\n","    \n","    def decode_idx_seq(self, l):\n","        return [self.tokens[i] for i in l]\n","    \n","    def encode_token_seq(self, l):\n","        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n","    \n","    def __len__(self):\n","        return len(self.tokens)\n"," \n","raw_datasets, datasets, vocab = load_wikitext(os.getcwd())\n","\n","data_loaders = {name: DataLoader(datasets[name], batch_size=32, shuffle=True,\n","                                     collate_fn=lambda x: pad_collate_fn(vocab.get_id('<pad>'), x))\n","                    for name in datasets}\n","wk2_train_dataloader = data_loaders['train']\n","wk2_val_dataloader = data_loaders['valid']\n","wk2_test_dataloader = data_loaders['test']"],"execution_count":6,"outputs":[{"output_type":"stream","text":["100%|██████████| 78274/78274 [02:00<00:00, 651.65it/s]\n","100%|██████████| 78274/78274 [00:00<00:00, 105856.78it/s]\n","100%|██████████| 8464/8464 [00:00<00:00, 124147.03it/s]\n","100%|██████████| 9708/9708 [00:00<00:00, 124416.17it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Vocab size: 33178\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kBWeRBxjl1jG","executionInfo":{"status":"ok","timestamp":1610222646176,"user_tz":300,"elapsed":120682,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}}},"source":["from torchtext.datasets import WikiText2\n","def init_wikitext_dataset():\n","    \"\"\"\n","    Fill in the details\n","    \"\"\"\n","    wikitext_val = wk2_val_dataloader\n","    wikitext_train = wk2_train_dataloader\n","    wikitext_test = wk2_test_dataloader\n","    \n","    return wikitext_train, wikitext_val, wikitext_test"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NmyifIDUl1jH"},"source":["#### Part B   \n","Here we design our own feature extractor. In MNIST that was a resnet because we were dealing with images. Now we need to pick a model that can model sequences better. Design an RNN-based model here."]},{"cell_type":"code","metadata":{"id":"C1cM0cIR_ds6"},"source":["import torch.nn as nn\n","\n","class GRU_LM(nn.Module):\n","    \"\"\"\n","    This model combines embedding, gru and projection layer into a single model\n","    \"\"\"\n","    def __init__(self, options):\n","        super().__init__()\n","        \n","        # create each LM part here \n","        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n","        self.gru = nn.GRU(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['gru_dropout'], batch_first=True)\n","        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n","        \n","    def forward(self, encoded_input_sequence):\n","        \"\"\"\n","        Forward method process the input from token ids to logits\n","        \"\"\"\n","        embeddings = self.lookup(encoded_input_sequence)\n","        #self.gru.flatten_parameters()\n","        gru_outputs = self.gru(embeddings)\n","        logits = self.projection(gru_outputs[0])\n","        \n","        return logits, gru_outputs[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rz46YkuSl1jI"},"source":["def init_feature_extractor():\n","    embedding_size = 300\n","    hidden_size = 1024\n","    num_layers = 2\n","    gru_dropout = 0.2\n","\n","    options = {\n","        'num_embeddings': len(vocab),\n","        'embedding_dim': embedding_size,\n","        'padding_idx': vocab.get_id('<pad>'),\n","        'input_size': embedding_size,\n","        'hidden_size': hidden_size,\n","        'num_layers': num_layers,\n","        'gru_dropout': gru_dropout,\n","    }\n","\n","    feature_extractor = GRU_LM((options)).to(device)\n","    return feature_extractor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kkQNvwCTl1jK"},"source":["#### Part C\n","Pretrain the feature extractor"]},{"cell_type":"code","metadata":{"id":"6utYWiIjPp24"},"source":["num_gpus = torch.cuda.device_count()\n","if num_gpus > 0:\n","    device = 'cuda'\n","else:\n","    device = 'cpu'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4l45nU_Ol1jK"},"source":["import numpy as np\n","import torch.optim as optim\n","from copy import deepcopy\n","def fit_feature_extractor(feature_extractor, wikitext_train, wikitext_val):\n","    best_feature_extractor = None\n","    lowest_loss = float('inf')\n","    criterion = nn.CrossEntropyLoss(ignore_index=vocab.get_id('<pad>'), reduction='sum')\n","    feature_extractor_parameters = [p for p in feature_extractor.parameters() if p.requires_grad]\n","    optimizer = optim.Adam(feature_extractor_parameters, lr=0.0001)\n","    for epoch_number in range(20):\n","      avg_loss=0\n","      feature_extractor.train()\n","      train_loss_cache = 0\n","      train_non_pad_tokens_cache = 0\n","      for i, (inp, target) in enumerate(wikitext_train):\n","          optimizer.zero_grad()\n","          inp = inp.to(device)\n","          target = target.to(device)\n","          logits = feature_extractor(inp)[0]\n","          loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n","          train_loss_cache += loss.item()\n","          non_pad_tokens = target.view(-1).ne(vocab.get_id('<pad>')).sum().item()\n","          train_non_pad_tokens_cache += non_pad_tokens\n","          loss /= non_pad_tokens\n","          loss.backward()\n","          optimizer.step()\n","          if i % 100 == 0:\n","              avg_loss = train_loss_cache / train_non_pad_tokens_cache\n","              print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n","              train_log_cache = []\n","            \n","      #do valid\n","      valid_loss_cache = 0\n","      valid_non_pad_tokens_cache = 0\n","      feature_extractor.eval()\n","      with torch.no_grad():\n","          for i, (inp, target) in enumerate(wikitext_val):\n","              inp = inp.to(device)\n","              target = target.to(device)\n","              logits = feature_extractor(inp)[0]\n","              loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n","              valid_loss_cache += loss.item()\n","              non_pad_tokens = target.view(-1).ne(vocab.get_id('<pad>')).sum().item()\n","              valid_non_pad_tokens_cache += non_pad_tokens\n","          avg_val_loss = valid_loss_cache / valid_non_pad_tokens_cache\n","          if avg_val_loss < lowest_loss:\n","              lowest_loss = avg_val_loss\n","              best_feature_extractor = deepcopy(feature_extractor)\n","          print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n","    return best_feature_extractor\n","          \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iv9Gz0W7Cx5c","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1571713675887,"user_tz":240,"elapsed":17036461,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"3d2ca1a6-1613-4f81-df6d-9e66b4334ca0"},"source":["wikitext_train, wikitext_val, wikitext_test = init_wikitext_dataset()\n","feature_extractor = fit_feature_extractor(init_feature_extractor(), wikitext_train, wikitext_val)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Step 0 avg train loss = 10.4097\n","Step 100 avg train loss = 7.9352\n","Step 200 avg train loss = 7.4803\n","Step 300 avg train loss = 7.2839\n","Step 400 avg train loss = 7.1609\n","Step 500 avg train loss = 7.0709\n","Step 600 avg train loss = 6.9968\n","Step 700 avg train loss = 6.9388\n","Step 800 avg train loss = 6.8864\n","Step 900 avg train loss = 6.8423\n","Step 1000 avg train loss = 6.8006\n","Step 1100 avg train loss = 6.7628\n","Step 1200 avg train loss = 6.7271\n","Step 1300 avg train loss = 6.6957\n","Step 1400 avg train loss = 6.6670\n","Step 1500 avg train loss = 6.6370\n","Step 1600 avg train loss = 6.6100\n","Step 1700 avg train loss = 6.5830\n","Step 1800 avg train loss = 6.5592\n","Step 1900 avg train loss = 6.5364\n","Step 2000 avg train loss = 6.5136\n","Step 2100 avg train loss = 6.4926\n","Step 2200 avg train loss = 6.4721\n","Step 2300 avg train loss = 6.4526\n","Step 2400 avg train loss = 6.4346\n","Validation loss after 0 epoch = 5.8344\n","Step 0 avg train loss = 5.8996\n","Step 100 avg train loss = 5.9147\n","Step 200 avg train loss = 5.9128\n","Step 300 avg train loss = 5.9145\n","Step 400 avg train loss = 5.9052\n","Step 500 avg train loss = 5.9006\n","Step 600 avg train loss = 5.8927\n","Step 700 avg train loss = 5.8880\n","Step 800 avg train loss = 5.8778\n","Step 900 avg train loss = 5.8718\n","Step 1000 avg train loss = 5.8655\n","Step 1100 avg train loss = 5.8591\n","Step 1200 avg train loss = 5.8529\n","Step 1300 avg train loss = 5.8466\n","Step 1400 avg train loss = 5.8416\n","Step 1500 avg train loss = 5.8375\n","Step 1600 avg train loss = 5.8316\n","Step 1700 avg train loss = 5.8251\n","Step 1800 avg train loss = 5.8184\n","Step 1900 avg train loss = 5.8115\n","Step 2000 avg train loss = 5.8055\n","Step 2100 avg train loss = 5.8001\n","Step 2200 avg train loss = 5.7957\n","Step 2300 avg train loss = 5.7911\n","Step 2400 avg train loss = 5.7855\n","Validation loss after 1 epoch = 5.5913\n","Step 0 avg train loss = 5.5850\n","Step 100 avg train loss = 5.5366\n","Step 200 avg train loss = 5.5306\n","Step 300 avg train loss = 5.5263\n","Step 400 avg train loss = 5.5281\n","Step 500 avg train loss = 5.5186\n","Step 600 avg train loss = 5.5181\n","Step 700 avg train loss = 5.5129\n","Step 800 avg train loss = 5.5103\n","Step 900 avg train loss = 5.5097\n","Step 1000 avg train loss = 5.5063\n","Step 1100 avg train loss = 5.5030\n","Step 1200 avg train loss = 5.5004\n","Step 1300 avg train loss = 5.4987\n","Step 1400 avg train loss = 5.4968\n","Step 1500 avg train loss = 5.4945\n","Step 1600 avg train loss = 5.4917\n","Step 1700 avg train loss = 5.4888\n","Step 1800 avg train loss = 5.4853\n","Step 1900 avg train loss = 5.4828\n","Step 2000 avg train loss = 5.4805\n","Step 2100 avg train loss = 5.4780\n","Step 2200 avg train loss = 5.4759\n","Step 2300 avg train loss = 5.4733\n","Step 2400 avg train loss = 5.4704\n","Validation loss after 2 epoch = 5.4504\n","Step 0 avg train loss = 5.2485\n","Step 100 avg train loss = 5.2318\n","Step 200 avg train loss = 5.2308\n","Step 300 avg train loss = 5.2355\n","Step 400 avg train loss = 5.2439\n","Step 500 avg train loss = 5.2409\n","Step 600 avg train loss = 5.2422\n","Step 700 avg train loss = 5.2446\n","Step 800 avg train loss = 5.2398\n","Step 900 avg train loss = 5.2404\n","Step 1000 avg train loss = 5.2415\n","Step 1100 avg train loss = 5.2405\n","Step 1200 avg train loss = 5.2384\n","Step 1300 avg train loss = 5.2382\n","Step 1400 avg train loss = 5.2380\n","Step 1500 avg train loss = 5.2357\n","Step 1600 avg train loss = 5.2344\n","Step 1700 avg train loss = 5.2329\n","Step 1800 avg train loss = 5.2322\n","Step 1900 avg train loss = 5.2310\n","Step 2000 avg train loss = 5.2303\n","Step 2100 avg train loss = 5.2302\n","Step 2200 avg train loss = 5.2292\n","Step 2300 avg train loss = 5.2274\n","Step 2400 avg train loss = 5.2257\n","Validation loss after 3 epoch = 5.3606\n","Step 0 avg train loss = 5.0297\n","Step 100 avg train loss = 5.0328\n","Step 200 avg train loss = 5.0272\n","Step 300 avg train loss = 5.0307\n","Step 400 avg train loss = 5.0259\n","Step 500 avg train loss = 5.0232\n","Step 600 avg train loss = 5.0257\n","Step 700 avg train loss = 5.0269\n","Step 800 avg train loss = 5.0238\n","Step 900 avg train loss = 5.0227\n","Step 1000 avg train loss = 5.0236\n","Step 1100 avg train loss = 5.0244\n","Step 1200 avg train loss = 5.0257\n","Step 1300 avg train loss = 5.0256\n","Step 1400 avg train loss = 5.0243\n","Step 1500 avg train loss = 5.0236\n","Step 1600 avg train loss = 5.0240\n","Step 1700 avg train loss = 5.0228\n","Step 1800 avg train loss = 5.0220\n","Step 1900 avg train loss = 5.0222\n","Step 2000 avg train loss = 5.0218\n","Step 2100 avg train loss = 5.0215\n","Step 2200 avg train loss = 5.0210\n","Step 2300 avg train loss = 5.0216\n","Step 2400 avg train loss = 5.0212\n","Validation loss after 4 epoch = 5.3120\n","Step 0 avg train loss = 4.8073\n","Step 100 avg train loss = 4.8130\n","Step 200 avg train loss = 4.8259\n","Step 300 avg train loss = 4.8343\n","Step 400 avg train loss = 4.8329\n","Step 500 avg train loss = 4.8321\n","Step 600 avg train loss = 4.8315\n","Step 700 avg train loss = 4.8325\n","Step 800 avg train loss = 4.8345\n","Step 900 avg train loss = 4.8341\n","Step 1000 avg train loss = 4.8339\n","Step 1100 avg train loss = 4.8359\n","Step 1200 avg train loss = 4.8364\n","Step 1300 avg train loss = 4.8397\n","Step 1400 avg train loss = 4.8385\n","Step 1500 avg train loss = 4.8404\n","Step 1600 avg train loss = 4.8404\n","Step 1700 avg train loss = 4.8391\n","Step 1800 avg train loss = 4.8400\n","Step 1900 avg train loss = 4.8408\n","Step 2000 avg train loss = 4.8418\n","Step 2100 avg train loss = 4.8423\n","Step 2200 avg train loss = 4.8427\n","Step 2300 avg train loss = 4.8423\n","Step 2400 avg train loss = 4.8423\n","Validation loss after 5 epoch = 5.2652\n","Step 0 avg train loss = 4.8019\n","Step 100 avg train loss = 4.6469\n","Step 200 avg train loss = 4.6574\n","Step 300 avg train loss = 4.6589\n","Step 400 avg train loss = 4.6551\n","Step 500 avg train loss = 4.6646\n","Step 600 avg train loss = 4.6651\n","Step 700 avg train loss = 4.6668\n","Step 800 avg train loss = 4.6664\n","Step 900 avg train loss = 4.6691\n","Step 1000 avg train loss = 4.6684\n","Step 1100 avg train loss = 4.6704\n","Step 1200 avg train loss = 4.6706\n","Step 1300 avg train loss = 4.6726\n","Step 1400 avg train loss = 4.6750\n","Step 1500 avg train loss = 4.6757\n","Step 1600 avg train loss = 4.6771\n","Step 1700 avg train loss = 4.6784\n","Step 1800 avg train loss = 4.6787\n","Step 1900 avg train loss = 4.6793\n","Step 2000 avg train loss = 4.6798\n","Step 2100 avg train loss = 4.6800\n","Step 2200 avg train loss = 4.6808\n","Step 2300 avg train loss = 4.6804\n","Step 2400 avg train loss = 4.6812\n","Validation loss after 6 epoch = 5.2399\n","Step 0 avg train loss = 4.5167\n","Step 100 avg train loss = 4.4967\n","Step 200 avg train loss = 4.4885\n","Step 300 avg train loss = 4.4924\n","Step 400 avg train loss = 4.4959\n","Step 500 avg train loss = 4.5015\n","Step 600 avg train loss = 4.5011\n","Step 700 avg train loss = 4.5020\n","Step 800 avg train loss = 4.5043\n","Step 900 avg train loss = 4.5065\n","Step 1000 avg train loss = 4.5080\n","Step 1100 avg train loss = 4.5127\n","Step 1200 avg train loss = 4.5122\n","Step 1300 avg train loss = 4.5136\n","Step 1400 avg train loss = 4.5156\n","Step 1500 avg train loss = 4.5180\n","Step 1600 avg train loss = 4.5198\n","Step 1700 avg train loss = 4.5226\n","Step 1800 avg train loss = 4.5241\n","Step 1900 avg train loss = 4.5251\n","Step 2000 avg train loss = 4.5262\n","Step 2100 avg train loss = 4.5284\n","Step 2200 avg train loss = 4.5295\n","Step 2300 avg train loss = 4.5303\n","Step 2400 avg train loss = 4.5318\n","Validation loss after 7 epoch = 5.2251\n","Step 0 avg train loss = 4.3967\n","Step 100 avg train loss = 4.3437\n","Step 200 avg train loss = 4.3519\n","Step 300 avg train loss = 4.3525\n","Step 400 avg train loss = 4.3578\n","Step 500 avg train loss = 4.3572\n","Step 600 avg train loss = 4.3628\n","Step 700 avg train loss = 4.3640\n","Step 800 avg train loss = 4.3658\n","Step 900 avg train loss = 4.3690\n","Step 1000 avg train loss = 4.3703\n","Step 1100 avg train loss = 4.3732\n","Step 1200 avg train loss = 4.3751\n","Step 1300 avg train loss = 4.3775\n","Step 1400 avg train loss = 4.3805\n","Step 1500 avg train loss = 4.3819\n","Step 1600 avg train loss = 4.3823\n","Step 1700 avg train loss = 4.3842\n","Step 1800 avg train loss = 4.3859\n","Step 1900 avg train loss = 4.3878\n","Step 2000 avg train loss = 4.3895\n","Step 2100 avg train loss = 4.3907\n","Step 2200 avg train loss = 4.3918\n","Step 2300 avg train loss = 4.3929\n","Step 2400 avg train loss = 4.3941\n","Validation loss after 8 epoch = 5.2178\n","Step 0 avg train loss = 3.9996\n","Step 100 avg train loss = 4.1895\n","Step 200 avg train loss = 4.2034\n","Step 300 avg train loss = 4.2097\n","Step 400 avg train loss = 4.2157\n","Step 500 avg train loss = 4.2189\n","Step 600 avg train loss = 4.2233\n","Step 700 avg train loss = 4.2264\n","Step 800 avg train loss = 4.2293\n","Step 900 avg train loss = 4.2335\n","Step 1000 avg train loss = 4.2349\n","Step 1100 avg train loss = 4.2376\n","Step 1200 avg train loss = 4.2416\n","Step 1300 avg train loss = 4.2451\n","Step 1400 avg train loss = 4.2456\n","Step 1500 avg train loss = 4.2483\n","Step 1600 avg train loss = 4.2502\n","Step 1700 avg train loss = 4.2516\n","Step 1800 avg train loss = 4.2537\n","Step 1900 avg train loss = 4.2564\n","Step 2000 avg train loss = 4.2583\n","Step 2100 avg train loss = 4.2605\n","Step 2200 avg train loss = 4.2618\n","Step 2300 avg train loss = 4.2631\n","Step 2400 avg train loss = 4.2638\n","Validation loss after 9 epoch = 5.2137\n","Step 0 avg train loss = 4.2596\n","Step 100 avg train loss = 4.0815\n","Step 200 avg train loss = 4.0905\n","Step 300 avg train loss = 4.0923\n","Step 400 avg train loss = 4.0928\n","Step 500 avg train loss = 4.1025\n","Step 600 avg train loss = 4.1035\n","Step 700 avg train loss = 4.1052\n","Step 800 avg train loss = 4.1073\n","Step 900 avg train loss = 4.1120\n","Step 1000 avg train loss = 4.1137\n","Step 1100 avg train loss = 4.1167\n","Step 1200 avg train loss = 4.1210\n","Step 1300 avg train loss = 4.1227\n","Step 1400 avg train loss = 4.1254\n","Step 1500 avg train loss = 4.1278\n","Step 1600 avg train loss = 4.1301\n","Step 1700 avg train loss = 4.1306\n","Step 1800 avg train loss = 4.1315\n","Step 1900 avg train loss = 4.1331\n","Step 2000 avg train loss = 4.1340\n","Step 2100 avg train loss = 4.1355\n","Step 2200 avg train loss = 4.1380\n","Step 2300 avg train loss = 4.1399\n","Step 2400 avg train loss = 4.1415\n","Validation loss after 10 epoch = 5.2672\n","Step 0 avg train loss = 4.0958\n","Step 100 avg train loss = 4.0020\n","Step 200 avg train loss = 3.9976\n","Step 300 avg train loss = 3.9951\n","Step 400 avg train loss = 3.9950\n","Step 500 avg train loss = 3.9990\n","Step 600 avg train loss = 4.0043\n","Step 700 avg train loss = 4.0002\n","Step 800 avg train loss = 4.0012\n","Step 900 avg train loss = 4.0006\n","Step 1000 avg train loss = 4.0027\n","Step 1100 avg train loss = 4.0050\n","Step 1200 avg train loss = 4.0073\n","Step 1300 avg train loss = 4.0089\n","Step 1400 avg train loss = 4.0105\n","Step 1500 avg train loss = 4.0113\n","Step 1600 avg train loss = 4.0132\n","Step 1700 avg train loss = 4.0158\n","Step 1800 avg train loss = 4.0167\n","Step 1900 avg train loss = 4.0186\n","Step 2000 avg train loss = 4.0205\n","Step 2100 avg train loss = 4.0218\n","Step 2200 avg train loss = 4.0243\n","Step 2300 avg train loss = 4.0258\n","Step 2400 avg train loss = 4.0278\n","Validation loss after 11 epoch = 5.2222\n","Step 0 avg train loss = 3.9631\n","Step 100 avg train loss = 3.8402\n","Step 200 avg train loss = 3.8475\n","Step 300 avg train loss = 3.8580\n","Step 400 avg train loss = 3.8605\n","Step 500 avg train loss = 3.8698\n","Step 600 avg train loss = 3.8744\n","Step 700 avg train loss = 3.8755\n","Step 800 avg train loss = 3.8793\n","Step 900 avg train loss = 3.8819\n","Step 1000 avg train loss = 3.8843\n","Step 1100 avg train loss = 3.8885\n","Step 1200 avg train loss = 3.8925\n","Step 1300 avg train loss = 3.8951\n","Step 1400 avg train loss = 3.8976\n","Step 1500 avg train loss = 3.8996\n","Step 1600 avg train loss = 3.9023\n","Step 1700 avg train loss = 3.9037\n","Step 1800 avg train loss = 3.9057\n","Step 1900 avg train loss = 3.9075\n","Step 2000 avg train loss = 3.9087\n","Step 2100 avg train loss = 3.9106\n","Step 2200 avg train loss = 3.9133\n","Step 2300 avg train loss = 3.9160\n","Step 2400 avg train loss = 3.9180\n","Validation loss after 12 epoch = 5.2353\n","Step 0 avg train loss = 3.9085\n","Step 100 avg train loss = 3.7548\n","Step 200 avg train loss = 3.7478\n","Step 300 avg train loss = 3.7530\n","Step 400 avg train loss = 3.7570\n","Step 500 avg train loss = 3.7614\n","Step 600 avg train loss = 3.7629\n","Step 700 avg train loss = 3.7662\n","Step 800 avg train loss = 3.7683\n","Step 900 avg train loss = 3.7753\n","Step 1000 avg train loss = 3.7773\n","Step 1100 avg train loss = 3.7813\n","Step 1200 avg train loss = 3.7848\n","Step 1300 avg train loss = 3.7856\n","Step 1400 avg train loss = 3.7886\n","Step 1500 avg train loss = 3.7925\n","Step 1600 avg train loss = 3.7957\n","Step 1700 avg train loss = 3.7982\n","Step 1800 avg train loss = 3.8002\n","Step 1900 avg train loss = 3.8028\n","Step 2000 avg train loss = 3.8051\n","Step 2100 avg train loss = 3.8072\n","Step 2200 avg train loss = 3.8092\n","Step 2300 avg train loss = 3.8107\n","Step 2400 avg train loss = 3.8134\n","Validation loss after 13 epoch = 5.2507\n","Step 0 avg train loss = 3.4000\n","Step 100 avg train loss = 3.6367\n","Step 200 avg train loss = 3.6444\n","Step 300 avg train loss = 3.6556\n","Step 400 avg train loss = 3.6581\n","Step 500 avg train loss = 3.6617\n","Step 600 avg train loss = 3.6677\n","Step 700 avg train loss = 3.6731\n","Step 800 avg train loss = 3.6739\n","Step 900 avg train loss = 3.6767\n","Step 1000 avg train loss = 3.6785\n","Step 1100 avg train loss = 3.6820\n","Step 1200 avg train loss = 3.6843\n","Step 1300 avg train loss = 3.6885\n","Step 1400 avg train loss = 3.6906\n","Step 1500 avg train loss = 3.6939\n","Step 1600 avg train loss = 3.6956\n","Step 1700 avg train loss = 3.6982\n","Step 1800 avg train loss = 3.7012\n","Step 1900 avg train loss = 3.7034\n","Step 2000 avg train loss = 3.7058\n","Step 2100 avg train loss = 3.7080\n","Step 2200 avg train loss = 3.7090\n","Step 2300 avg train loss = 3.7113\n","Step 2400 avg train loss = 3.7129\n","Validation loss after 14 epoch = 5.2614\n","Step 0 avg train loss = 3.4693\n","Step 100 avg train loss = 3.5485\n","Step 200 avg train loss = 3.5544\n","Step 300 avg train loss = 3.5619\n","Step 400 avg train loss = 3.5615\n","Step 500 avg train loss = 3.5633\n","Step 600 avg train loss = 3.5693\n","Step 700 avg train loss = 3.5738\n","Step 800 avg train loss = 3.5781\n","Step 900 avg train loss = 3.5819\n","Step 1000 avg train loss = 3.5821\n","Step 1100 avg train loss = 3.5859\n","Step 1200 avg train loss = 3.5882\n","Step 1300 avg train loss = 3.5909\n","Step 1400 avg train loss = 3.5938\n","Step 1500 avg train loss = 3.5963\n","Step 1600 avg train loss = 3.5987\n","Step 1700 avg train loss = 3.6010\n","Step 1800 avg train loss = 3.6041\n","Step 1900 avg train loss = 3.6066\n","Step 2000 avg train loss = 3.6087\n","Step 2100 avg train loss = 3.6109\n","Step 2200 avg train loss = 3.6139\n","Step 2300 avg train loss = 3.6162\n","Step 2400 avg train loss = 3.6183\n","Validation loss after 15 epoch = 5.2771\n","Step 0 avg train loss = 3.6025\n","Step 100 avg train loss = 3.4465\n","Step 200 avg train loss = 3.4599\n","Step 300 avg train loss = 3.4615\n","Step 400 avg train loss = 3.4637\n","Step 500 avg train loss = 3.4672\n","Step 600 avg train loss = 3.4688\n","Step 700 avg train loss = 3.4732\n","Step 800 avg train loss = 3.4779\n","Step 900 avg train loss = 3.4819\n","Step 1000 avg train loss = 3.4842\n","Step 1100 avg train loss = 3.4883\n","Step 1200 avg train loss = 3.4907\n","Step 1300 avg train loss = 3.4943\n","Step 1400 avg train loss = 3.4984\n","Step 1500 avg train loss = 3.5015\n","Step 1600 avg train loss = 3.5056\n","Step 1700 avg train loss = 3.5086\n","Step 1800 avg train loss = 3.5109\n","Step 1900 avg train loss = 3.5142\n","Step 2000 avg train loss = 3.5164\n","Step 2100 avg train loss = 3.5190\n","Step 2200 avg train loss = 3.5220\n","Step 2300 avg train loss = 3.5245\n","Step 2400 avg train loss = 3.5270\n","Validation loss after 16 epoch = 5.2973\n","Step 0 avg train loss = 3.2648\n","Step 100 avg train loss = 3.3721\n","Step 200 avg train loss = 3.3714\n","Step 300 avg train loss = 3.3728\n","Step 400 avg train loss = 3.3778\n","Step 500 avg train loss = 3.3860\n","Step 600 avg train loss = 3.3918\n","Step 700 avg train loss = 3.3962\n","Step 800 avg train loss = 3.3974\n","Step 900 avg train loss = 3.4003\n","Step 1000 avg train loss = 3.4053\n","Step 1100 avg train loss = 3.4079\n","Step 1200 avg train loss = 3.4098\n","Step 1300 avg train loss = 3.4129\n","Step 1400 avg train loss = 3.4173\n","Step 1500 avg train loss = 3.4198\n","Step 1600 avg train loss = 3.4217\n","Step 1700 avg train loss = 3.4243\n","Step 1800 avg train loss = 3.4253\n","Step 1900 avg train loss = 3.4277\n","Step 2000 avg train loss = 3.4304\n","Step 2100 avg train loss = 3.4328\n","Step 2200 avg train loss = 3.4356\n","Step 2300 avg train loss = 3.4384\n","Step 2400 avg train loss = 3.4398\n","Validation loss after 17 epoch = 5.3205\n","Step 0 avg train loss = 3.2367\n","Step 100 avg train loss = 3.2852\n","Step 200 avg train loss = 3.2778\n","Step 300 avg train loss = 3.2829\n","Step 400 avg train loss = 3.2899\n","Step 500 avg train loss = 3.2949\n","Step 600 avg train loss = 3.2994\n","Step 700 avg train loss = 3.3023\n","Step 800 avg train loss = 3.3067\n","Step 900 avg train loss = 3.3110\n","Step 1000 avg train loss = 3.3151\n","Step 1100 avg train loss = 3.3204\n","Step 1200 avg train loss = 3.3228\n","Step 1300 avg train loss = 3.3256\n","Step 1400 avg train loss = 3.3281\n","Step 1500 avg train loss = 3.3323\n","Step 1600 avg train loss = 3.3358\n","Step 1700 avg train loss = 3.3379\n","Step 1800 avg train loss = 3.3405\n","Step 1900 avg train loss = 3.3435\n","Step 2000 avg train loss = 3.3463\n","Step 2100 avg train loss = 3.3495\n","Step 2200 avg train loss = 3.3522\n","Step 2300 avg train loss = 3.3550\n","Step 2400 avg train loss = 3.3580\n","Validation loss after 18 epoch = 5.3439\n","Step 0 avg train loss = 3.0393\n","Step 100 avg train loss = 3.1935\n","Step 200 avg train loss = 3.2047\n","Step 300 avg train loss = 3.2169\n","Step 400 avg train loss = 3.2204\n","Step 500 avg train loss = 3.2225\n","Step 600 avg train loss = 3.2225\n","Step 700 avg train loss = 3.2264\n","Step 800 avg train loss = 3.2328\n","Step 900 avg train loss = 3.2360\n","Step 1000 avg train loss = 3.2394\n","Step 1100 avg train loss = 3.2399\n","Step 1200 avg train loss = 3.2443\n","Step 1300 avg train loss = 3.2480\n","Step 1400 avg train loss = 3.2509\n","Step 1500 avg train loss = 3.2525\n","Step 1600 avg train loss = 3.2552\n","Step 1700 avg train loss = 3.2576\n","Step 1800 avg train loss = 3.2598\n","Step 1900 avg train loss = 3.2625\n","Step 2000 avg train loss = 3.2656\n","Step 2100 avg train loss = 3.2686\n","Step 2200 avg train loss = 3.2718\n","Step 2300 avg train loss = 3.2748\n","Step 2400 avg train loss = 3.2768\n","Validation loss after 19 epoch = 5.3649\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C6oGPJ-NzRA6"},"source":["torch.save(feature_extractor.state_dict(), \"/content/drive/My Drive/feature_extractor\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EZLMUnsjhZiZ","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1571777696426,"user_tz":240,"elapsed":2780,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"554018a4-88c9-4abf-d027-b02898671fd5"},"source":["feature_extractor = init_feature_extractor()\n","feature_extractor.load_state_dict(torch.load(\"/content/drive/My Drive/feature_extractor\"))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"markdown","metadata":{"id":"r_lcRC4hl1jM"},"source":["#### Part D\n","Calculate the test perplexity on wikitext2. Feel free to recycle code from previous assignments from this class. "]},{"cell_type":"code","metadata":{"id":"4Mx1HqRTl1jN"},"source":["def calculate_wiki2_test_perplexity(feature_extractor, wikitext_test):\n","    test_loss_cache = 0\n","    test_non_pad_tokens_cache = 0\n","    criterion = nn.CrossEntropyLoss(ignore_index=vocab.get_id('<pad>'), reduction='sum')\n","    feature_extractor.eval()\n","    with torch.no_grad():\n","          for i, (inp, target) in enumerate(wikitext_test):\n","              inp = inp.to(device)\n","              target = target.to(device)\n","              logits = feature_extractor(inp)[0]\n","              loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n","              test_loss_cache += loss.item()\n","              non_pad_tokens = target.view(-1).ne(vocab.get_id('<pad>')).sum().item()\n","              test_non_pad_tokens_cache += non_pad_tokens\n","          avg_test_loss = test_loss_cache / test_non_pad_tokens_cache\n","          test_ppl = 2**(avg_test_loss/np.log(2))\n","    return test_ppl"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ZJt9K1bl1jP"},"source":["#### Let's grade your results!\n","(don't touch this part)"]},{"cell_type":"code","metadata":{"id":"8C8uxbeel1jP","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1571715321963,"user_tz":240,"elapsed":32869,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"60d55c4d-90d1-41c9-d82c-a7807d3c5469"},"source":["def grade_wikitext2():\n","    # load data\n","    wikitext_train, wikitext_val, wikitext_test = init_wikitext_dataset()\n","\n","    # load feature extractor\n","    #feature_extractor = init_feature_extractor()\n","\n","    # pretrain using the feature extractor\n","    #feature_extractor = fit_feature_extractor(feature_extractor, wikitext_train, wikitext_val)\n","\n","    # check test accuracy\n","    test_ppl = calculate_wiki2_test_perplexity(feature_extractor, wikitext_test)\n","\n","    # the real threshold will be released by Oct 11 \n","    assert test_ppl < 10000, 'ummm... your perplexity is too high...'\n","    print(test_ppl)\n","grade_wikitext2()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["173.39674515304023\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FpX_RRral1jR"},"source":["---   \n","## Question 3 (fine-tune on MNLI)\n","In this question you will use your feature_extractor from question 2\n","to fine-tune on MNLI.\n","\n","(From the website):\n","The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation. The corpus served as the basis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.\n","\n","MNLI has 3 genres (3 classes).\n","The goal of this question is to maximize the test accuracy in MNLI. "]},{"cell_type":"markdown","metadata":{"id":"AdPXiHzsl1jS"},"source":["### Part A\n","In this section you need to generate the training, validation and test split. Feel free to use code from your previous lectures."]},{"cell_type":"code","metadata":{"id":"3QSpKaf_iIcz"},"source":["y_label_map = {'contradiction':0,'neutral':1,'entailment':2}\n","import pandas as pd\n","import io\n","from collections import Counter\n","\n","def get_string_tokenized_data(data):\n","    \n","    tokenized_data_x = [];\n","    y_labels = []\n","    all_tokens = [];\n","    \n","    for i,x in enumerate(data):\n","        label = x[2]\n","        if label == 'nan':\n","          continue\n","        \n","        label = y_label_map[label]\n","        y_labels.append(label)\n","        \n","        dp = [x[0].split(), x[1].split()]\n","        tokenized_data_x.append(dp)\n","        all_tokens += (dp[0] + dp[1])\n","        \n","\n","    return all_tokens, tokenized_data_x, y_labels\n","        \n","\n","\n","# convert token to id in the dataset\n","def token2index_dataset(tokens_data, token2id):\n","    indices_data = []\n","    for tokens1, tokens2 in tokens_data:\n","        index_list1 = [token2id[token] if token in token2id else UNK_IDX for token in tokens1]\n","        index_list2 = [token2id[token] if token in token2id else UNK_IDX for token in tokens2]\n","        indices_data.append([index_list1, index_list2])\n","    return indices_data\n","  \n","# convert token to id in the dataset\n","def token2index_using_wikitext2_dict(tokens_data, vocab):\n","    indices_data = []\n","    for tokens1, tokens2 in tokens_data:\n","        index_list1 = vocab.encode_token_seq(tokens1)\n","        index_list2 = vocab.encode_token_seq(tokens2)\n","        indices_data.append([index_list1, index_list2])\n","    return indices_data\n","\n","\n","def build_vocab(all_tokens):\n","    # Returns:\n","    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n","    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n","    token_counter = Counter(all_tokens)\n","    vocab, count = zip(*token_counter.most_common(MAX_VOCAB_SIZE))\n","    id2token = list(vocab)\n","    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n","    id2token = ['<pad>', '<unk>'] + id2token\n","    token2id['<pad>'] = PAD_IDX \n","    token2id['<unk>'] = UNK_IDX\n","    return token2id, id2token"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hJJT0Z-5k6rO","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1571838975488,"user_tz":240,"elapsed":4663,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"b5bd25ad-581c-406b-d9c5-24e413fbd79b"},"source":["# LOAD VAL\n","val_df = pd.read_csv('/content/drive/My Drive/mnli_val.tsv', sep=\"\\t\")\n","print(val_df.head(2))\n","\n","val_df  = np.array(val_df)\n","val_df = val_df.astype(str)\n","val_genre_list = val_df[:, 3]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["                                           sentence1  ...      genre\n","0  'Not entirely , ' I snapped , harsher than int...  ...    fiction\n","1  cook and then the next time it would be my tur...  ...  telephone\n","\n","[2 rows x 4 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yIP-tPxSmLy2"},"source":["_, val_data_x, val_data_y = get_string_tokenized_data(val_df)\n","del val_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wN0Izj1NmO93"},"source":["train_df = pd.read_csv('/content/drive/My Drive/mnli_train.tsv', sep=\"\\t\", chunksize=5000)\n","train_genre_list = []\n","train_data_x = []\n","train_data_y = []\n","\n","for chunk in train_df:\n","\n","  chunk  = np.array(chunk)\n","  chunk = chunk.astype(str)\n","  train_genre_list_chunk = chunk[:, 3]\n","  train_genre_list.append(train_genre_list_chunk)\n","  \n","  _, train_data_x_chunk, train_data_y_chunk = get_string_tokenized_data(chunk)\n","  train_data_x.append(train_data_x_chunk)\n","  train_data_y.append(train_data_y_chunk)\n","\n","train_genre_list = np.concatenate(train_genre_list)\n","train_data_x = np.concatenate(train_data_x)\n","train_data_y = np.concatenate(train_data_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OtefzcMUmPNi","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1571838996199,"user_tz":240,"elapsed":25328,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"5c4e7a1c-5b30-4c48-e873-7f1bbbe46999"},"source":["mnli_raw_datasets = {'train': train_data_x, 'val': val_data_x}\n","mnli_tokenized_datasets = tokenize_mnli_dataset(mnli_raw_datasets, vocab)\n","\n","train_data_indices = mnli_tokenized_datasets['train']\n","val_data_indices = mnli_tokenized_datasets['val']\n","\n","\n","# double checking\n","print('\\n')\n","print (\"Train dataset size is {}\".format(len(train_data_indices)))\n","print (\"Val dataset size is {}\".format(len(val_data_indices)))\n","\n","# del train_data_x\n","# del train_data_y\n","# del val_data_x\n","# del val_data_y\n","del mnli_tokenized_datasets"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 20000/20000 [00:00<00:00, 62570.08it/s]\n","100%|██████████| 5000/5000 [00:00<00:00, 67275.49it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n","Train dataset size is 20000\n","Val dataset size is 5000\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"cwKaqY2lmPLw"},"source":["unique_genre = list(set(val_genre_list));\n","nb_classes = len(y_label_map)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPUakpgomPJ7"},"source":["MAX_SENTENCE_LENGTH = 200\n","class MNLIDataset(Dataset):\n","    \"\"\"\n","    Class that represents a train/validation/test dataset that's readable for PyTorch\n","    Note that this class inherits torch.utils.data.Dataset\n","    \"\"\"\n","\n","    def __init__(self, data_x, target_list):\n","        \"\"\"\n","        @param data_list: list of newsgroup tokens\n","        @param target_list: list of newsgroup targets\n","\n","        \"\"\"\n","        self.data_x = data_x;\n","        self.target_list = target_list\n","        \n","        assert(len(data_x) == len(target_list))\n","\n","    def __len__(self):\n","        return len(self.target_list)\n","\n","    def __getitem__(self, key):\n","        \"\"\"\n","        Triggered when you call dataset[i]\n","        \"\"\"\n","        prem_token_idx = self.data_x[key][0][:MAX_SENTENCE_LENGTH]\n","        hyp_token_idx = self.data_x[key][1][:MAX_SENTENCE_LENGTH]\n","        label = self.target_list[key]\n","        return [prem_token_idx, hyp_token_idx, label]\n","\n","\n","def encode_collate_func(batch):\n","    \"\"\"\n","    Customized function for DataLoader that dynamically pads the batch so that all\n","    data have the same length\n","    \"\"\"\n","    prem_data_list = []\n","    hyp_data_list = []\n","    label_list = []\n","    length_list = []\n","    # print(\"collate batch: \", batch[0][0])\n","    # batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n","    for datum in batch:\n","        label_list.append(datum[2])\n","    # padding\n","    for datum in batch:\n","        prem_padded_vec = np.pad(np.array(datum[0]),\n","                                 pad_width=((0, MAX_SENTENCE_LENGTH - len(datum[0]))),\n","                                 mode=\"constant\", constant_values=0)\n","        hyp_padded_vec = np.pad(np.array(datum[1]),\n","                                pad_width=((0, MAX_SENTENCE_LENGTH - len(datum[1]))),\n","                                mode=\"constant\", constant_values=0)\n","        prem_data_list.append(prem_padded_vec)\n","        hyp_data_list.append(hyp_padded_vec)\n","    return [torch.from_numpy((np.array(prem_data_list))), torch.from_numpy(np.array(hyp_data_list)),\n","            torch.LongTensor(label_list)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VjQRNHa8mPHl","colab":{"base_uri":"https://localhost:8080/","height":236},"executionInfo":{"status":"error","timestamp":1571794004378,"user_tz":240,"elapsed":891,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"e35d8285-ba47-4756-caff-23b4456dadbd"},"source":["BATCH_SIZE = 32\n","nb_train_samples = int(0.95 * len(train_data_indices))\n","nb_val_samples = len(train_data_indices) - nb_train_samples\n","\n","# train/val split\n","train_val_dataset = MNLIDataset(train_data_indices, train_data_y)\n","train_dataset, val_dataset = random_split(train_val_dataset, [nb_train_samples, nb_val_samples])\n","\n","# train loader\n","train_mnli_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=BATCH_SIZE,\n","                                           collate_fn=encode_collate_func,\n","                                           shuffle=True)\n","\n","# val loader\n","val_mnli_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n","                                           batch_size=BATCH_SIZE,\n","                                           collate_fn=encode_collate_func,\n","                                           shuffle=True)\n","\n","# test loader\n","test_dataset = MNLIDataset(val_data_indices, val_data_y)\n","test_mnli_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                           batch_size=BATCH_SIZE,\n","                                           collate_fn=encode_collate_func,\n","                                           shuffle=True)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-a2aff277c862>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnb_train_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.95\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnb_val_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnb_train_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# train/val split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_data_indices' is not defined"]}]},{"cell_type":"code","metadata":{"id":"OXMovAIymPE7","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1571777493563,"user_tz":240,"elapsed":1434,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"ee91e0a8-1170-48ce-d374-270ec13939f4"},"source":["print('MNLI dataset statistics')\n","print(f'training samples:{len(train_mnli_loader)}')\n","print(f'val samples:{len(val_mnli_loader)}')\n","print(f'test samples:{len(test_mnli_loader)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MNLI dataset statistics\n","training samples:594\n","val samples:32\n","test samples:157\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7GF_-4g5l1jU"},"source":["from torchtext.datasets import MultiNLI\n","\n","def init_mnli_dataset():\n","    \"\"\"\n","    Fill in the details\n","    \"\"\"\n","    mnli_val = val_mnli_loader\n","    mnli_train = train_mnli_loader\n","    mnli_test = test_mnli_loader\n","    \n","    return mnli_train, mnli_val, mnli_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y90Gy7psl1jY"},"source":["### Part B\n","Here we again design a model for finetuning. Use the output of your feature-extractor as the input to this model. This should be a powerful classifier (up to you)."]},{"cell_type":"code","metadata":{"id":"_EutFefmtw_U"},"source":["import torch.nn as nn\n","\n","class MultiNLI_Classifier(nn.Module):\n","    def __init__(self, options):\n","        super().__init__()\n","        # create each LM part here \n","        self.fc1 = nn.Linear(768, 256)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Linear(256, options['num_classes'])\n","        \n","    def forward(self, feature_extractor_output):\n","        feature_extractor_output = feature_extractor_output.view(feature_extractor_output.size(0), -1)\n","        output = self.fc1(feature_extractor_output)\n","        output = self.relu(output)\n","        output = self.fc2(output)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2BGmN849l1jY"},"source":["def init_finetune_model():\n","    options = {\n","        #'hidden_size': 1024,\n","        'num_classes': 3,\n","    }\n","    fine_tune_model = MultiNLI_Classifier(options).to(device)\n","    return fine_tune_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OR2aleocl1ja"},"source":["### Part C\n","Use the feature_extractor and your fine_tune_model to fine_tune MNLI"]},{"cell_type":"code","metadata":{"id":"JOXCNVoQl1ja"},"source":["def fine_tune_mnli(feature_extractor, fine_tune_model, mnli_train, mnli_val):\n","    # YOUR CODE HERE\n","    highest_acc = 0\n","    best_feature_extractor = feature_extractor\n","    best_classifier = None\n","    for param in feature_extractor.parameters():\n","        param.requires_grad = True\n","    criterion = nn.CrossEntropyLoss(ignore_index=vocab.get_id('<pad>'), reduction='sum')\n","    fine_tune_model_parameters = [p for p in fine_tune_model.parameters() if p.requires_grad]\n","    feature_extractor_parameters = [p for p in feature_extractor.parameters() if p.requires_grad]\n","    optimizer = torch.optim.Adam(list(fine_tune_model_parameters + feature_extractor_parameters), lr = 0.0001)\n","    #do train\n","    for epoch in range(10):\n","        fine_tune_model.train()\n","        correct_train = 0\n","        total_train = 0\n","        correct_val = 0\n","        total_val = 0\n","        for i, (inp1, inp2, target) in enumerate(mnli_train):\n","            optimizer.zero_grad()\n","            inp1 = inp1.to(device)\n","            inp2 = inp2.to(device)\n","            target = target.to(device)\n","            feature_extractor_output1, feature_extractor_output2 = feature_extractor(inp1)[1], feature_extractor(inp2)[1]\n","            feature_extractor_output = torch.cat([feature_extractor_output1, feature_extractor_output2], dim=1)\n","            outputs = fine_tune_model(feature_extractor_output)\n","            loss = criterion(outputs, target)\n","            loss.backward()\n","            optimizer.step()\n","            predicted = outputs.max(1, keepdim=True)[1]\n","            total_train += target.size(0)\n","            correct_train += predicted.eq(target.view_as(predicted)).sum().item()\n","        print('Training accuracy after {} epoch = {:.{prec}f}'.format(epoch, 100 * correct_train / total_train, prec=4))\n","        # do eval\n","        feature_extractor.eval()\n","        fine_tune_model.eval()\n","        with torch.no_grad():\n","            for i, (inp1, inp2, target) in enumerate(mnli_val):\n","                optimizer.zero_grad()\n","                inp1 = inp1.to(device)\n","                inp2 = inp2.to(device)\n","                target = target.to(device)\n","                feature_extractor_output1, feature_extractor_output2 = feature_extractor(inp1)[1], feature_extractor(inp2)[1]\n","                feature_extractor_output = torch.cat([feature_extractor_output1, feature_extractor_output2], dim=1)\n","                outputs = fine_tune_model(feature_extractor_output)\n","                loss = criterion(outputs, target)\n","                predicted = outputs.max(1, keepdim=True)[1]\n","                total_val += target.size(0)\n","                correct_val += predicted.eq(target.view_as(predicted)).sum().item()\n","            if correct_val/total_val > highest_acc:\n","                highest_acc = correct_val/total_val\n","                best_feature_extractor = deepcopy(feature_extractor)\n","                best_classifier = deepcopy(fine_tune_model)\n","        print('Validation accuracy after {} epoch = {:.{prec}f}'.format(epoch, 100 * correct_val / total_val, prec=4))\n","    return best_feature_extractor, best_classifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2e-c-wGel1jb"},"source":["### Part D\n","Evaluate the test accuracy"]},{"cell_type":"code","metadata":{"id":"tv_1T53Ol1jc"},"source":["def calculate_mnli_test_accuracy(feature_extractor, fine_tune_model, mnli_test):\n","    correct_test = 0\n","    total_test = 0\n","    criterion = nn.CrossEntropyLoss(ignore_index=vocab.get_id('<pad>'), reduction='sum')\n","    feature_extractor.eval()\n","    fine_tune_model.eval()\n","    with torch.no_grad():\n","        for i, (inp1, inp2, target) in enumerate(mnli_test):\n","            optimizer.zero_grad()\n","            inp1 = inp1.to(device)\n","            inp2 = inp2.to(device)\n","            target = target.to(device)\n","            feature_extractor_output1, feature_extractor_output2 = feature_extractor(inp1)[1], feature_extractor(inp2)[1]\n","            feature_extractor_output = torch.cat([feature_extractor_output1, feature_extractor_output2], dim=1)\n","            outputs = fine_tune_model(feature_extractor_output)\n","            loss = criterion(outputs, target)\n","            predicted = outputs.max(1, keepdim=True)[1]\n","            total_test += target.size(0)\n","            correct_test += predicted.eq(target.view_as(predicted)).sum().item()\n","    return correct_test/total_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J65K_0bel1je"},"source":["### Let's grade your results"]},{"cell_type":"code","metadata":{"id":"0D8ET3dQl1je","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1571777705185,"user_tz":240,"elapsed":1821,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"714d57a9-20fc-48b3-e17d-772b3e07574b"},"source":["def grade_mnli():\n","    # load data\n","    mnli_train, mnli_val, mnli_test = init_mnli_dataset()\n","\n","    # no need to load feature extractor because it is fine-tuned\n","    feature_extractor = init_feature_extractor()\n","\n","    # init the fine_tune model\n","    fine_tune_model = init_finetune_model()\n","    \n","    # finetune\n","    fine_tune_mnli(feature_extractor, fine_tune_model, mnli_train, mnli_val)\n","\n","    # check test accuracy\n","    test_accuracy = calculate_mnli_test_accuracy(feature_extractor, wikitext_test)\n","\n","    # the real threshold will be released by Oct 11 \n","    assert test_ppl > 0.00, 'ummm... your accuracy is too low...'\n","    \n","grade_mnli()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-57ec831ef762>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mtest_ppl\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.00\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ummm... your accuracy is too low...'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mgrade_mnli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-40-57ec831ef762>\u001b[0m in \u001b[0;36mgrade_mnli\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# finetune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mfine_tune_mnli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfine_tune_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnli_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnli_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# check test accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-a25fb0703fd3>\u001b[0m in \u001b[0;36mfine_tune_mnli\u001b[0;34m(feature_extractor, fine_tune_model, mnli_train, mnli_val)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mfeature_extractor_output1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extractor_output2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tune_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_extractor_output1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extractor_output2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-e35cb8c0092c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, feature_extractor_output1, feature_extractor_output2)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_extractor_output1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extractor_output2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [32 x 409600], m2: [2048 x 256] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:290"]}]},{"cell_type":"markdown","metadata":{"id":"xuaWo_hfl1jg"},"source":["---  \n","### Question 4 (BERT)\n","\n","A major direction in research came from a model called BERT, released last year.  \n","\n","In this question you'll use BERT as your feature_extractor instead of the model you\n","designed yourself.\n","\n","To get BERT, head on over to (https://github.com/huggingface/transformers) and load your BERT model here"]},{"cell_type":"code","metadata":{"id":"N8QIh1tIl1jg","colab":{"base_uri":"https://localhost:8080/","height":377},"executionInfo":{"status":"ok","timestamp":1571839271119,"user_tz":240,"elapsed":3229,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"c2a8ebfa-7873-4776-af48-83e4e8c79136"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.1.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.35)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.16.5)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.9.251)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.83)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.8.19)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: botocore<1.13.0,>=1.12.251 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.12.251)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.251->boto3->transformers) (2.5.3)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.251->boto3->transformers) (0.15.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KOEsqRPVl1ji"},"source":["### Part A (init BERT)\n","In this section you need to create an instance of BERT and return if from the function"]},{"cell_type":"code","metadata":{"id":"o1qpl5AjR4p2"},"source":["from transformers.data.processors.glue import MnliProcessor\n","import torch\n","import pandas as pd\n","import os\n","import sys\n","import shutil\n","import argparse\n","import tempfile\n","import urllib.request\n","import zipfile\n","from transformers import glue_convert_examples_to_features as convert_examples_to_features\n","from transformers import BertTokenizer\n","from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n","\n","\n","from transformers import (\n","    BertModel,\n","    BertTokenizer\n",")\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n","bert = BertModel.from_pretrained('bert-base-cased', output_attentions=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0q2wnT7OR4s0"},"source":["TASKS = [\"CoLA\", \"SST\", \"MRPC\", \"QQP\", \"STS\", \"MNLI\", \"SNLI\", \"QNLI\", \"RTE\", \"WNLI\", \"diagnostic\"]\n","TASK2PATH = {\n","    \"CoLA\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FCoLA.zip?alt=media&token=46d5e637-3411-4188-bc44-5809b5bfb5f4\",  # noqa\n","    \"SST\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8\",  # noqa\n","    \"MRPC\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc\",  # noqa\n","    \"QQP\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP-clean.zip?alt=media&token=11a647cb-ecd3-49c9-9d31-79f8ca8fe277\",  # noqa\n","    \"STS\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSTS-B.zip?alt=media&token=bddb94a7-8706-4e0d-a694-1109e12273b5\",  # noqa\n","    \"MNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&token=50329ea1-e339-40e2-809c-10c40afff3ce\",  # noqa\n","    \"SNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSNLI.zip?alt=media&token=4afcfbb2-ff0c-4b2d-a09a-dbf07926f4df\",  # noqa\n","    \"QNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLIv2.zip?alt=media&token=6fdcf570-0fc5-4631-8456-9505272d1601\",  # noqa\n","    \"RTE\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb\",  # noqa\n","    \"WNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FWNLI.zip?alt=media&token=068ad0a0-ded7-4bd7-99a5-5e00222e0faf\",  # noqa\n","    \"diagnostic\": [\n","        \"https://storage.googleapis.com/mtl-sentence-representations.appspot.com/tsvsWithoutLabels%2FAX.tsv?GoogleAccessId=firebase-adminsdk-0khhl@mtl-sentence-representations.iam.gserviceaccount.com&Expires=2498860800&Signature=DuQ2CSPt2Yfre0C%2BiISrVYrIFaZH1Lc7hBVZDD4ZyR7fZYOMNOUGpi8QxBmTNOrNPjR3z1cggo7WXFfrgECP6FBJSsURv8Ybrue8Ypt%2FTPxbuJ0Xc2FhDi%2BarnecCBFO77RSbfuz%2Bs95hRrYhTnByqu3U%2FYZPaj3tZt5QdfpH2IUROY8LiBXoXS46LE%2FgOQc%2FKN%2BA9SoscRDYsnxHfG0IjXGwHN%2Bf88q6hOmAxeNPx6moDulUF6XMUAaXCSFU%2BnRO2RDL9CapWxj%2BDl7syNyHhB7987hZ80B%2FwFkQ3MEs8auvt5XW1%2Bd4aCU7ytgM69r8JDCwibfhZxpaa4gd50QXQ%3D%3D\",  # noqa\n","        \"https://www.dropbox.com/s/ju7d95ifb072q9f/diagnostic-full.tsv?dl=1\",\n","    ],\n","}\n","\n","MRPC_TRAIN = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\"\n","MRPC_TEST = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt\"\n","\n","\n","def download_and_extract(task, data_dir):\n","    print(\"Downloading and extracting %s...\" % task)\n","    data_file = \"%s.zip\" % task\n","    urllib.request.urlretrieve(TASK2PATH[task], data_file)\n","    with zipfile.ZipFile(data_file) as zip_ref:\n","        zip_ref.extractall(data_dir)\n","    os.remove(data_file)\n","    print(\"\\tCompleted!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WMxXF_1hR4xG","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1571839289637,"user_tz":240,"elapsed":12923,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"99e8a08f-3f60-468f-f621-433b83d0762f"},"source":["download_and_extract('MNLI', '.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading and extracting MNLI...\n","\tCompleted!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YOWlaiTkR419"},"source":["processor = MnliProcessor()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YUe1fHXSR4z-"},"source":["\n","def generate_mnli_bert_dataloaders():\n","  # ----------------------\n","  # TRAIN/VAL DATALOADERS\n","  # ----------------------\n","  train = processor.get_train_examples('MNLI')\n","  features = convert_examples_to_features(train,\n","                                          tokenizer,\n","                                          label_list=['contradiction','neutral','entailment'],\n","                                          max_length=128,\n","                                          output_mode='classification',\n","                                          pad_on_left=False,\n","                                          pad_token=tokenizer.pad_token_id,\n","                                          pad_token_segment_id=0)\n","  train_dataset = TensorDataset(torch.tensor([f.input_ids for f in features], dtype=torch.long), \n","                                torch.tensor([f.attention_mask for f in features], dtype=torch.long), \n","                                torch.tensor([f.token_type_ids for f in features], dtype=torch.long), \n","                                torch.tensor([f.label for f in features], dtype=torch.long))\n","\n","  nb_train_samples = int(0.95 * len(train_dataset))\n","  nb_val_samples = len(train_dataset) - nb_train_samples\n","\n","  bert_mnli_train_dataset, bert_mnli_val_dataset = random_split(train_dataset, [nb_train_samples, nb_val_samples])\n","\n","  # train loader\n","  train_sampler = RandomSampler(bert_mnli_train_dataset)\n","  bert_mnli_train_dataloader = DataLoader(bert_mnli_train_dataset, sampler=train_sampler, batch_size=32)\n","\n","  # val loader\n","  val_sampler = RandomSampler(bert_mnli_val_dataset)\n","  bert_mnli_val_dataloader = DataLoader(bert_mnli_val_dataset, sampler=val_sampler, batch_size=32)\n","\n","\n","  # ----------------------\n","  # TEST DATALOADERS\n","  # ----------------------\n","  dev = processor.get_dev_examples('MNLI')\n","  features = convert_examples_to_features(dev,\n","                                          tokenizer,\n","                                          label_list=['contradiction','neutral','entailment'],\n","                                          max_length=128,\n","                                          output_mode='classification',\n","                                          pad_on_left=False,\n","                                          pad_token=tokenizer.pad_token_id,\n","                                          pad_token_segment_id=0)\n","\n","  bert_mnli_test_dataset = TensorDataset(torch.tensor([f.input_ids for f in features], dtype=torch.long), \n","                                torch.tensor([f.attention_mask for f in features], dtype=torch.long), \n","                                torch.tensor([f.token_type_ids for f in features], dtype=torch.long), \n","                                torch.tensor([f.label for f in features], dtype=torch.long))\n","\n","  # test dataset\n","  test_sampler = RandomSampler(bert_mnli_test_dataset)\n","  bert_mnli_test_dataloader = DataLoader(bert_mnli_test_dataset, sampler=test_sampler, batch_size=32)\n","  \n","  return bert_mnli_train_dataloader, bert_mnli_val_dataloader, bert_mnli_test_dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K2wEdSSqR4vj"},"source":["bert_mnli_train_dataloader, bert_mnli_val_dataloader, bert_mnli_test_dataloader = generate_mnli_bert_dataloaders()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q6wyWfvfQqqc"},"source":["class BERTSequence(nn.Module):\n","    def __init__(self, bert):\n","        super().__init__()\n","        self.bert = bert\n","        \n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        h, _, attn = self.bert(input_ids=input_ids, \n","                               attention_mask=attention_mask, \n","                               token_type_ids=token_type_ids)\n","        h_cls = h[:, 0]\n","        return h_cls, attn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QpRmUdMzl1jj"},"source":["from transformers import BertTokenizer, BertModel, BertForMaskedLM\n","\n","def init_bert():\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n","    bert = BertModel.from_pretrained('bert-base-cased', output_attentions=True)   \n","    BERT = BERTSequence(bert)\n","    return BERT"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QFh0UdY3s6GY"},"source":["device = \"cuda\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CNfEobP9r56e"},"source":["BERT_feature_extractor = init_bert()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vtezb0JrsRv0","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1571839300596,"user_tz":240,"elapsed":336,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"86bf0412-db73-4e13-aa0c-911f5363a7c1"},"source":["BERT_feature_extractor"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BERTSequence(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"0cPDzPwQr-WG"},"source":["BERT_feature_extractor = BERT_feature_extractor.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bFyGFlvil1jk"},"source":["## Part B (fine-tune with BERT)\n","\n","Use BERT as your feature extractor to finetune MNLI. Use a new finetune model (reset weights)."]},{"cell_type":"code","metadata":{"id":"QoZ7-UpQSs3p"},"source":["from copy import deepcopy\n","def fine_tune_mnli_BERT(BERT_feature_extractor, fine_tune_model, bert_mnli_train_dataloader, bert_mnli_val_dataloader):   \n","    highest_acc = 0\n","    best_feature_extractor = BERT_feature_extractor\n","    best_classifier = None\n","    BERT_feature_extractor = BERT_feature_extractor.to(device)\n","    fine_tune_model = fine_tune_model.to(device)\n","    for param in BERT_feature_extractor.parameters():\n","        param.requires_grad = False\n","    criterion = nn.CrossEntropyLoss()\n","    fine_tune_model_parameters = [p for p in fine_tune_model.parameters() if p.requires_grad]\n","    optimizer = torch.optim.Adam(fine_tune_model_parameters, lr = 0.0001)\n","    #do train\n","    for epoch in range(5):\n","        BERT_feature_extractor.train()\n","        fine_tune_model.train()\n","        correct_train = 0\n","        total_train = 0\n","        correct_val = 0\n","        total_val = 0\n","        for i, (inp, att_mask, token_type_ids, target) in enumerate(bert_mnli_train_dataloader):\n","            optimizer.zero_grad()\n","            inp, att_mask, token_type_ids, target = inp.to(device), att_mask.to(device), token_type_ids.to(device), target.to(device)\n","            feature_extractor_output = BERT_feature_extractor(inp, att_mask, token_type_ids)[0]\n","            outputs = fine_tune_model(feature_extractor_output)\n","            loss = criterion(outputs, target)\n","            loss.backward()\n","            optimizer.step()\n","            predicted = outputs.max(1, keepdim=True)[1]\n","            total_train += target.size(0)\n","            correct_train += predicted.eq(target.view_as(predicted)).sum().item()\n","        print('Training accuracy after {} epoch = {:.{prec}f}'.format(epoch, 100 * correct_train / total_train, prec=4))\n","        # do eval\n","        BERT_feature_extractor.eval()\n","        fine_tune_model.eval()\n","        with torch.no_grad():\n","            for i, (inp, att_mask, token_type_ids, target) in enumerate(bert_mnli_val_dataloader):\n","                optimizer.zero_grad()\n","                inp, att_mask, token_type_ids, target = inp.to(device), att_mask.to(device), token_type_ids.to(device), target.to(device)\n","                feature_extractor_output = BERT_feature_extractor(inp, att_mask, token_type_ids)[0]\n","                outputs = fine_tune_model(feature_extractor_output)\n","                loss = criterion(outputs, target)\n","                predicted = outputs.max(1, keepdim=True)[1]\n","                total_val += target.size(0)\n","                correct_val += predicted.eq(target.view_as(predicted)).sum().item()\n","            if correct_val/total_val > highest_acc:\n","                highest_acc = correct_val/total_val\n","                #best_feature_extractor = deepcopy(BERT_feature_extractor)\n","                best_classifier = deepcopy(fine_tune_model)\n","        print('Validation accuracy after {} epoch = {:.{prec}f}'.format(epoch, 100 * correct_val / total_val, prec=4))\n","    return best_feature_extractor, best_classifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-8aedXnel1jo"},"source":["## Part C\n","Evaluate how well we did"]},{"cell_type":"code","metadata":{"id":"_UEnUOvul1jp"},"source":["def calculate_mnli_test_accuracy_BERT(feature_extractor, fine_tune_model, mnli_test):\n","    \n","    # YOUR CODE HERE...\n","    correct_test = 0\n","    total_test = 0\n","    criterion = nn.CrossEntropyLoss()\n","    feature_extractor.eval()\n","    fine_tune_model.eval()\n","    with torch.no_grad():\n","        for i, (inp, att_mask, token_type_ids, target) in enumerate(bert_mnli_test_dataloader):\n","            optimizer.zero_grad()\n","            inp, att_mask, token_type_ids, target = inp.to(device), att_mask.to(device), token_type_ids.to(device), target.to(device)\n","            feature_extractor_output = BERT_feature_extractor(inp, att_mask, token_type_ids)[0]\n","            outputs = fine_tune_model(feature_extractor_output)\n","            loss = criterion(outputs, target)\n","            predicted = outputs.max(1, keepdim=True)[1]\n","            total_test += target.size(0)\n","            correct_test += predicted.eq(target.view_as(predicted)).sum().item()\n","    return correct_test/total_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E-iissQ5l1jq"},"source":["## Let's grade your BERT results!"]},{"cell_type":"code","metadata":{"id":"yoDZjCORl1jr","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"error","timestamp":1571793291194,"user_tz":240,"elapsed":3157977,"user":{"displayName":"Nyutian Long","photoUrl":"","userId":"12521528341538869501"}},"outputId":"4593b1f2-8000-4851-e429-5de6239ed07e"},"source":["def grade_mnli_BERT():\n","    BERT_feature_extractor = init_bert()\n","    \n","    # load data\n","    mnli_train, mnli_val, mnli_test = bert_mnli_train_dataloader, bert_mnli_val_dataloader, bert_mnli_test_dataloader\n","\n","    # init the fine_tune model\n","    fine_tune_model = init_finetune_model()\n","    \n","    # finetune\n","    fine_tune_mnli_BERT(BERT_feature_extractor, fine_tune_model, mnli_train, mnli_val)\n","\n","    # check test accuracy\n","    test_accuracy = calculate_mnli_test_accuracy(BERT_feature_extractor, mnli_test)\n","    \n","    # the real threshold will be released by Oct 11 \n","    assert test_ppl > 0.0, 'ummm... your accuracy is too low...'\n","    \n","grade_mnli_BERT()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training accuracy after 0 epoch = 51.5584\n","Validation accuracy after 0 epoch = 55.0163\n","Training accuracy after 1 epoch = 53.6993\n","Validation accuracy after 1 epoch = 54.7056\n","Training accuracy after 2 epoch = 54.3212\n","Validation accuracy after 2 epoch = 56.9719\n"],"name":"stdout"}]}]}